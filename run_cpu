#!/bin/bash

#Usage : ./run_cpu <game_num>
#game_num is 1, 2, 3 ...
#----------------------------------

#specify the output folder for logging stats and graphs
exp_folder="logs/run1_cpu/"

mkdir -p $exp_folder;

STEP_SIZE=1000 #step size for learn_start, eval, saving, etc.
hist_len=1 #Number of previous states to consider
max_steps=20 #max steps  in a single episode
minibatch_size=64 #batch size of transitions used in updates
quest_levels=1
game_num=$1
agent="NeuralQLearner"
n_replay=1

#Set this to 1 if playing the Fantasy world
tutorial_world=0
text_world_location="/home/taey16/text-world/"

#Baselines and for analysis
random_test=0
analyze_test=0

#Set this to 1 to use the RNN/LSTM for representation generation
recurrent=1

#Set this to 1 (along with bow_embedding for netfile)
# to use bigram representation
bigram=0

# The models below each represent the type of representation used.
# One of these options MUST be used.
#-------------------------------
#netfile="\"bow_embedding\""  # BOW-DQN or BI-DQN
#netfile="\"bow_embedding_simple\""  #BOW-LIN or BI-LIN
#netfile="\"rnn_embedding\""   #RNN-DQN
netfile="\"lstm_embedding\""  #LSTM=DQN

# You can also start by loading a model from a previous run
# netfile="\"logs/run1\""

#params for DQN
update_freq=4
actrep=1
discount=0.5
seed=1
learn_start=$STEP_SIZE
initial_priority="false"
replay_memory=100000
eps=1
eps_end=0.2
eps_endt=200000
lr=0.0005

agent_type=$exp_folder"DQN"
agent_name=$agent_type
state_dim=100
ncols=1
agent_params="lr="$lr",ep="$eps",ep_end="$eps_end",ep_endt="$eps_endt",discount="$discount",hist_len="$hist_len",learn_start="$learn_start",replay_memory="$replay_memory",update_freq="$update_freq",n_replay="$n_replay",network="$netfile",state_dim="$state_dim",minibatch_size="$minibatch_size",rescale_r=1,ncols="$ncols",bufferSize=512,valid_size=500,target_q="$STEP_SIZE",clip_delta=1,min_reward=-1,max_reward=10"
steps=2000000
eval_freq=$STEP_SIZE
eval_steps=$STEP_SIZE
prog_freq=$STEP_SIZE
save_freq=$STEP_SIZE
gpu=-1
random_starts=30
num_threads=1

args="-recurrent $recurrent -name $agent_name -agent $agent -agent_params $agent_params -steps $steps -eval_freq $eval_freq -eval_steps $eval_steps -prog_freq $prog_freq -save_freq $save_freq -actrep $actrep -gpu $gpu -random_starts $random_starts -seed $seed -threads $num_threads"
echo $args

OMP_NUM_THREADS=1 th agent.lua $args -saveNetworkParams -exp_folder $exp_folder  -game_num $game_num -quest_levels $quest_levels -state_dim  $state_dim -max_steps $max_steps -tutorial_world $tutorial_world -random_test $random_test -analyze_test $analyze_test -text_world_location $text_world_location;
